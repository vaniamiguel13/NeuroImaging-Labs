{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neuroimaging 2025/2026**\n",
    "\n",
    "**Project 10: Multimodal Age Group Classification**\n",
    "\n",
    "Tutors: Beatriz Vale & Vânia Miguel\n",
    "\n",
    "Created by: [Your Names Here]\n",
    "\n",
    "---\n",
    "\n",
    "> **Dataset:** MPI-Leipzig Mind-Brain-Body (LEMON)\n",
    ">\n",
    "> **Modalities:** MRI + DWI + fMRI + EEG\n",
    ">\n",
    "> **Subjects:** 10 (5 young, 5 old)\n",
    ">\n",
    "> **Goal:** Extract features from each modality using techniques from Labs 1-5, train a simple classifier, and compare which modalities are most informative.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **Important:** With only 10 subjects, we do NOT expect high classification accuracy. The focus of this project is on the **process**: correctly applying each pipeline, extracting meaningful features, and critically discussing the results and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objectives**\n",
    "```\n",
    "* Apply structural MRI processing (Lab 1) to extract subcortical volumes with FIRST\n",
    "* Apply diffusion MRI processing (Lab 2) to extract FA from white matter tracts\n",
    "* Apply fMRI connectivity analysis (Lab 4) to extract functional connectivity features\n",
    "* Apply EEG spectral analysis (Lab 5) to extract band power and alpha peak frequency\n",
    "* Integrate multimodal features and train a simple classifier (young vs old)\n",
    "* Compare unimodal vs multimodal classification performance\n",
    "* Critically discuss results and limitations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I. Set up the environment**\n",
    "\n",
    "We need both Neurodesk (for FSL and MRtrix3) and MNE-Python (for EEG). We also need scikit-learn for the classification step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Neurodesk\n",
    "%%capture\n",
    "import os\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.environ[\"LD_PRELOAD\"] = \"\";\n",
    "    os.environ[\"APPTAINER_BINDPATH\"] = \"/content,/tmp,/cvmfs\"\n",
    "    os.environ[\"MPLCONFIGDIR\"] = \"/content/matplotlib-mpldir\"\n",
    "    os.environ[\"LMOD_CMD\"] = \"/usr/share/lmod/lmod/libexec/lmod\"\n",
    "\n",
    "    !curl -J -O https://raw.githubusercontent.com/NeuroDesk/neurocommand/main/googlecolab_setup.sh\n",
    "    !chmod +x googlecolab_setup.sh\n",
    "    !./googlecolab_setup.sh\n",
    "\n",
    "    os.environ[\"MODULEPATH\"] = ':'.join(map(str, list(map(lambda x: os.path.join(os.path.abspath('/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/'), x), os.listdir('/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neuroimaging tools\n",
    "import lmod\n",
    "await lmod.load('fsl/6.0.7.16')\n",
    "await lmod.load('mrtrix3/3.0.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python packages for EEG and ML\n",
    "!pip3 install -q mne==1.9.0 mne_icalabel==0.7.0 scikit-learn pandas seaborn pymatreader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Download LEMON data**\n",
    "\n",
    "We select 10 subjects: 5 young (20-35y) and 5 old (59-77y), all with T1w, DWI, fMRI, and EEG available.\n",
    "\n",
    "**Data sources:**\n",
    "- **MRI / fMRI / DWI:** OpenNeuro ds000221 (ses-01)\n",
    "- **EEG:** GWDG FTP or NITRC-INDI (separate download)\n",
    "- **ID mapping:** CSV file to match INDI IDs ↔ original LEMON IDs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Select subjects\n",
    "\n",
    "Before downloading, you need to identify subjects that have **all 4 modalities**. Use the data availability CSV from the LEMON dataset page to find matching subjects.\n",
    "\n",
    "⚠️ The EEG data uses different subject IDs than OpenNeuro. Use the ID mapping CSV from the INDI page to match them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our 10 subjects (replace with your actual subject IDs after checking availability)\n",
    "# These are example IDs from OpenNeuro ds000221 (ses-01)\n",
    "young_subjects = ['sub-010002', 'sub-010003', 'sub-010004', 'sub-010005', 'sub-010006']\n",
    "old_subjects   = ['sub-010017', 'sub-010018', 'sub-010019', 'sub-010020', 'sub-010021']\n",
    "all_subjects   = young_subjects + old_subjects\n",
    "labels         = [0]*5 + [1]*5  # 0 = young, 1 = old\n",
    "label_names    = ['Young']*5 + ['Old']*5\n",
    "\n",
    "print(f\"Total subjects: {len(all_subjects)}\")\n",
    "print(f\"Young: {len(young_subjects)}, Old: {len(old_subjects)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download MRI / fMRI / DWI from OpenNeuro\n",
    "\n",
    "Using DataLad to download only the specific subjects and sessions we need.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DataLad and clone the dataset (metadata only, no data yet)\n",
    "!pip install -q datalad\n",
    "!git config --global user.name \"Student\"\n",
    "!git config --global user.email \"student@example.com\"\n",
    "\n",
    "# Clone the dataset (this only downloads metadata, ~fast)\n",
    "!datalad install https://github.com/OpenNeuroDatasets/ds000221.git\n",
    "os.chdir('ds000221')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for each subject (ses-01 only: LEMON protocol)\n",
    "# This downloads T1w, fMRI, and DWI for each subject\n",
    "for sub in all_subjects:\n",
    "    print(f\"\\n--- Downloading {sub} ---\")\n",
    "    !datalad get {sub}/ses-01/anat/ -J 4\n",
    "    !datalad get {sub}/ses-01/func/ -J 4\n",
    "    !datalad get {sub}/ses-01/dwi/ -J 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download EEG data\n",
    "\n",
    "EEG data is hosted separately. Download from the LEMON EEG page.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download EEG data for matching subjects\n",
    "# Replace URLs with the actual LEMON EEG download links for your subjects\n",
    "# The EEG data is organized per subject as .tar.gz files\n",
    "\n",
    "EEG_BASE_URL = \"https://ftp.gwdg.de/pub/misc/MPI-Leipzig_Mind-Brain-Body-LEMON/EEG_MPILMBB_LEMON/EEG_Raw_BIDS_ID/\"\n",
    "\n",
    "os.makedirs('/content/eeg_data', exist_ok=True)\n",
    "\n",
    "# NOTE: You need to map OpenNeuro IDs to LEMON EEG IDs using the CSV file\n",
    "# Example (replace with your actual mapped IDs):\n",
    "eeg_ids = {\n",
    "    'sub-010002': 'sub-010002',  # Replace with actual EEG ID mapping\n",
    "    # ... add all 10 subjects\n",
    "}\n",
    "\n",
    "# Download EEG for each subject (adapt URL pattern to actual structure)\n",
    "for sub, eeg_id in eeg_ids.items():\n",
    "    print(f\"Downloading EEG for {sub} ({eeg_id})...\")\n",
    "    # !wget -q -P /content/eeg_data/ {EEG_BASE_URL}/{eeg_id}.tar.gz\n",
    "    # !tar xzf /content/eeg_data/{eeg_id}.tar.gz -C /content/eeg_data/\n",
    "    \n",
    "print(\"⚠️ Adapt the download commands above to the actual EEG file structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Verify data availability\n",
    "\n",
    "Check that all expected files exist for each subject before proceeding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data files exist for each subject\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Subject':<15} {'T1w':>5} {'fMRI':>5} {'DWI':>5} {'EEG':>5}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for sub in all_subjects:\n",
    "    has_t1  = os.path.exists(f'{sub}/ses-01/anat/')\n",
    "    has_func = os.path.exists(f'{sub}/ses-01/func/')\n",
    "    has_dwi = os.path.exists(f'{sub}/ses-01/dwi/')\n",
    "    has_eeg = os.path.exists(f'/content/eeg_data/{sub}/')\n",
    "    \n",
    "    status = lambda x: '  ✓' if x else '  ✗'\n",
    "    print(f\"{sub:<15} {status(has_t1):>5} {status(has_func):>5} {status(has_dwi):>5} {status(has_eeg):>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III. MRI Structural Features (Lab 1)**\n",
    "\n",
    "We extract **subcortical volumes** using FIRST (as learned in Lab 1).\n",
    "\n",
    "**Pipeline:** Brain extraction (BET) → Subcortical segmentation (FIRST) → Volume extraction (fslstats)\n",
    "\n",
    "**Features extracted per subject:** Volume of L/R thalamus, hippocampus, caudate, putamen, pallidum, amygdala, accumbens (14 values).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Brain extraction and FIRST segmentation\n",
    "\n",
    "Apply BET and run_first_all for each subject, exactly as in Lab 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each subject: BET + FIRST\n",
    "mri_features = {}\n",
    "\n",
    "for sub in all_subjects:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {sub} - Structural MRI\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    anat_dir = f'{sub}/ses-01/anat/'\n",
    "    # Find T1w file (naming may vary)\n",
    "    t1_file = !ls {anat_dir}/*T1w.nii.gz 2>/dev/null | head -1\n",
    "    t1_file = t1_file[0] if t1_file else None\n",
    "    \n",
    "    if not t1_file:\n",
    "        print(f\"  ⚠️ No T1w found for {sub}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  T1w file: {t1_file}\")\n",
    "    \n",
    "    # Create working directory\n",
    "    work_dir = f'/content/mri_work/{sub}'\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Brain extraction with BET (Lab 1)\n",
    "    print(\"  Step 1: Brain extraction (BET)...\")\n",
    "    !bet {t1_file} {work_dir}/T1_brain -R -f 0.5\n",
    "    \n",
    "    # Step 2: FIRST subcortical segmentation (Lab 1)\n",
    "    print(\"  Step 2: Subcortical segmentation (FIRST)...\")\n",
    "    !run_first_all -i {work_dir}/T1_brain -b -o {work_dir}/T1_first\n",
    "    \n",
    "    print(f\"  ✓ Done: {sub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract subcortical volumes\n",
    "\n",
    "Use fslstats to extract the volume (in mm³) of each subcortical structure, as in Lab 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subcortical structures segmented by FIRST\n",
    "structures = [\n",
    "    'L_Thal', 'R_Thal', 'L_Hipp', 'R_Hipp',\n",
    "    'L_Caud', 'R_Caud', 'L_Puta', 'R_Puta',\n",
    "    'L_Pall', 'R_Pall', 'L_Amyg', 'R_Amyg',\n",
    "    'L_Accu', 'R_Accu'\n",
    "]\n",
    "\n",
    "mri_features = {}\n",
    "\n",
    "for sub in all_subjects:\n",
    "    work_dir = f'/content/mri_work/{sub}'\n",
    "    seg_file = f'{work_dir}/T1_first_all_fast_firstseg.nii.gz'\n",
    "    \n",
    "    if not os.path.exists(seg_file):\n",
    "        print(f\"⚠️ FIRST output missing for {sub}\")\n",
    "        mri_features[sub] = [np.nan] * len(structures)\n",
    "        continue\n",
    "    \n",
    "    volumes = []\n",
    "    for i, struct in enumerate(structures):\n",
    "        # FIRST labels are indexed starting from specific values\n",
    "        # Extract volume of non-zero voxels for each structure mesh\n",
    "        mesh_file = f'{work_dir}/T1_first-{struct}_first.nii.gz'\n",
    "        if os.path.exists(mesh_file):\n",
    "            result = !fslstats {mesh_file} -V\n",
    "            vol_mm3 = float(result[0].split()[1]) if result else np.nan\n",
    "        else:\n",
    "            vol_mm3 = np.nan\n",
    "        volumes.append(vol_mm3)\n",
    "    \n",
    "    mri_features[sub] = volumes\n",
    "    print(f\"{sub}: extracted {len([v for v in volumes if not np.isnan(v)])} structure volumes\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_mri = pd.DataFrame(mri_features, index=structures).T\n",
    "df_mri.index.name = 'Subject'\n",
    "print(\"\\nMRI Feature Matrix:\")\n",
    "df_mri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. QC: Visualize one example\n",
    "\n",
    "Always check results visually!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FIRST segmentation for one subject\n",
    "example_sub = all_subjects[0]\n",
    "work_dir = f'/content/mri_work/{example_sub}'\n",
    "\n",
    "from nilearn import plotting\n",
    "\n",
    "# Plot subcortical segmentation overlaid on T1\n",
    "plotting.plot_roi(\n",
    "    f'{work_dir}/T1_first_all_fast_firstseg.nii.gz',\n",
    "    bg_img=f'{work_dir}/T1_brain.nii.gz',\n",
    "    title=f'FIRST Segmentation - {example_sub}',\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(0, -20, 10)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. Diffusion MRI Features (Lab 2)**\n",
    "\n",
    "We extract **FA (Fractional Anisotropy)** values from major white matter tracts using the JHU atlas.\n",
    "\n",
    "**Pipeline:** Denoising (dwidenoise) → Preprocessing (dwifslpreproc) → Tensor fitting (dwi2tensor + tensor2metric) → FA extraction from JHU ROIs\n",
    "\n",
    "**Features extracted per subject:** Mean FA in 5 major tracts (genu CC, body CC, splenium CC, SLF, corticospinal tract) = 5 values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DWI preprocessing and tensor fitting\n",
    "\n",
    "Apply the preprocessing pipeline from Lab 2 to each subject.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each subject: DWI preprocessing + tensor fitting\n",
    "dwi_features = {}\n",
    "\n",
    "for sub in all_subjects:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {sub} - Diffusion MRI\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    dwi_dir = f'{sub}/ses-01/dwi/'\n",
    "    dwi_file = !ls {dwi_dir}/*dwi.nii.gz 2>/dev/null | head -1\n",
    "    dwi_file = dwi_file[0] if dwi_file else None\n",
    "    \n",
    "    if not dwi_file:\n",
    "        print(f\"  ⚠️ No DWI found for {sub}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    bval_file = dwi_file.replace('.nii.gz', '.bval')\n",
    "    bvec_file = dwi_file.replace('.nii.gz', '.bvec')\n",
    "    \n",
    "    work_dir = f'/content/dwi_work/{sub}'\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Denoising (Lab 2)\n",
    "    print(\"  Step 1: Denoising (dwidenoise)...\")\n",
    "    !dwidenoise {dwi_file} {work_dir}/dwi_den.mif -fslgrad {bvec_file} {bval_file} -force\n",
    "    \n",
    "    # Step 2: Preprocessing (Lab 2) \n",
    "    # Note: simplified version without fieldmaps\n",
    "    print(\"  Step 2: Preprocessing (dwifslpreproc)...\")\n",
    "    !dwifslpreproc {work_dir}/dwi_den.mif {work_dir}/dwi_preproc.mif -rpe_none -pe_dir AP -force\n",
    "    \n",
    "    # Step 3: Brain mask\n",
    "    print(\"  Step 3: Brain mask (dwi2mask)...\")\n",
    "    !dwi2mask {work_dir}/dwi_preproc.mif {work_dir}/mask.mif -force\n",
    "    \n",
    "    # Step 4: Tensor fitting and FA/MD maps (Lab 2)\n",
    "    print(\"  Step 4: Tensor fitting (dwi2tensor)...\")\n",
    "    !dwi2tensor {work_dir}/dwi_preproc.mif {work_dir}/dt.mif -mask {work_dir}/mask.mif -force\n",
    "    !tensor2metric {work_dir}/dt.mif -fa {work_dir}/fa.mif -mask {work_dir}/mask.mif -force\n",
    "    \n",
    "    # Convert to NIfTI for fslstats\n",
    "    !mrconvert {work_dir}/fa.mif {work_dir}/fa.nii.gz -force\n",
    "    \n",
    "    print(f\"  ✓ Done: {sub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract FA from JHU white matter atlas ROIs\n",
    "\n",
    "Use the JHU atlas from FSL to extract mean FA in major tracts, using fslstats as in Lab 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JHU White Matter Atlas tract labels (selected major tracts)\n",
    "# These correspond to specific label indices in the JHU-ICBM atlas\n",
    "jhu_tracts = {\n",
    "    'Genu_CC': 3,        # Genu of corpus callosum\n",
    "    'Body_CC': 4,        # Body of corpus callosum  \n",
    "    'Splenium_CC': 5,    # Splenium of corpus callosum\n",
    "    'CST_R': 8,          # Corticospinal tract R\n",
    "    'SLF_R': 20,         # Superior longitudinal fasciculus R\n",
    "}\n",
    "\n",
    "# JHU atlas location in FSL\n",
    "jhu_atlas = os.path.join(os.environ.get('FSLDIR', ''), \n",
    "                         'data/atlases/JHU/JHU-ICBM-labels-1mm.nii.gz')\n",
    "\n",
    "print(f\"JHU Atlas: {jhu_atlas}\")\n",
    "print(f\"Exists: {os.path.exists(jhu_atlas)}\")\n",
    "\n",
    "dwi_features = {}\n",
    "\n",
    "for sub in all_subjects:\n",
    "    work_dir = f'/content/dwi_work/{sub}'\n",
    "    fa_file = f'{work_dir}/fa.nii.gz'\n",
    "    \n",
    "    if not os.path.exists(fa_file):\n",
    "        print(f\"⚠️ FA map missing for {sub}\")\n",
    "        dwi_features[sub] = [np.nan] * len(jhu_tracts)\n",
    "        continue\n",
    "    \n",
    "    # Register FA to MNI space for atlas overlap\n",
    "    # Using FLIRT (linear registration, as in Lab 1)\n",
    "    mni_template = os.path.join(os.environ.get('FSLDIR', ''),\n",
    "                                'data/standard/MNI152_T1_1mm_brain.nii.gz')\n",
    "    \n",
    "    !flirt -in {fa_file} -ref {mni_template} -out {work_dir}/fa_mni.nii.gz \\\n",
    "           -omat {work_dir}/fa2mni.mat -dof 12\n",
    "    \n",
    "    fa_vals = []\n",
    "    for tract_name, label_idx in jhu_tracts.items():\n",
    "        # Create tract mask from JHU atlas\n",
    "        !fslmaths {jhu_atlas} -thr {label_idx} -uthr {label_idx} -bin {work_dir}/tract_mask.nii.gz\n",
    "        \n",
    "        # Extract mean FA within tract\n",
    "        result = !fslstats {work_dir}/fa_mni.nii.gz -k {work_dir}/tract_mask.nii.gz -M\n",
    "        mean_fa = float(result[0]) if result else np.nan\n",
    "        fa_vals.append(mean_fa)\n",
    "    \n",
    "    dwi_features[sub] = fa_vals\n",
    "    print(f\"{sub}: FA extracted from {len(jhu_tracts)} tracts\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_dwi = pd.DataFrame(dwi_features, index=list(jhu_tracts.keys())).T\n",
    "df_dwi.index.name = 'Subject'\n",
    "print(\"\\nDWI Feature Matrix:\")\n",
    "df_dwi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. QC: Visualize FA map for one subject\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FA map for one subject\n",
    "example_sub = all_subjects[0]\n",
    "work_dir = f'/content/dwi_work/{example_sub}'\n",
    "\n",
    "from nilearn import plotting\n",
    "\n",
    "plotting.plot_anat(\n",
    "    f'{work_dir}/fa.nii.gz',\n",
    "    title=f'FA Map - {example_sub}',\n",
    "    display_mode='ortho',\n",
    "    cmap='hot',\n",
    "    vmin=0, vmax=1\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **V. Functional Connectivity Features (Lab 4)**\n",
    "\n",
    "We extract **functional connectivity** features from resting-state fMRI using the Schaefer atlas.\n",
    "\n",
    "**Pipeline:** Basic preprocessing (motion correction + smoothing) → Parcellation (Schaefer 100) → FC matrix → Network-level summary features\n",
    "\n",
    "**Features extracted per subject:** Mean intra-network and inter-network connectivity for 7 Yeo networks = ~28 values (7 intra + 21 inter).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic fMRI preprocessing\n",
    "\n",
    "Apply minimal preprocessing: motion correction and smoothing, as in Lab 3 Part 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each subject: basic fMRI preprocessing\n",
    "for sub in all_subjects:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {sub} - fMRI\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    func_dir = f'{sub}/ses-01/func/'\n",
    "    func_file = !ls {func_dir}/*bold.nii.gz 2>/dev/null | head -1\n",
    "    func_file = func_file[0] if func_file else None\n",
    "    \n",
    "    if not func_file:\n",
    "        print(f\"  ⚠️ No fMRI found for {sub}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    work_dir = f'/content/fmri_work/{sub}'\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    \n",
    "    # Check dimensions\n",
    "    !fslinfo {func_file} | grep -E \"^dim|^pixdim\"\n",
    "    \n",
    "    # Step 1: Motion correction (Lab 3)\n",
    "    print(\"  Step 1: Motion correction (mcflirt)...\")\n",
    "    !mcflirt -in {func_file} -out {work_dir}/func_mc -plots\n",
    "    \n",
    "    # Step 2: Brain extraction on mean functional\n",
    "    print(\"  Step 2: Skull stripping (bet)...\")\n",
    "    !fslmaths {work_dir}/func_mc -Tmean {work_dir}/func_mean\n",
    "    !bet {work_dir}/func_mean {work_dir}/func_brain -f 0.3 -m\n",
    "    \n",
    "    # Step 3: Spatial smoothing (Lab 3, FWHM=6mm -> sigma=2.548)\n",
    "    print(\"  Step 3: Smoothing (fslmaths -s)...\")\n",
    "    !fslmaths {work_dir}/func_mc -s 2.548 {work_dir}/func_smooth\n",
    "    \n",
    "    # Step 4: Temporal filtering (bandpass 0.01-0.1 Hz)\n",
    "    # Get TR for this subject\n",
    "    tr_result = !fslval {func_file} pixdim4\n",
    "    tr = float(tr_result[0]) if tr_result else 2.0\n",
    "    hp_sigma = int(1.0 / (2 * 0.01 * tr))  # High-pass sigma in volumes\n",
    "    lp_sigma = int(1.0 / (2 * 0.1 * tr))   # Low-pass sigma in volumes\n",
    "    \n",
    "    print(f\"  Step 4: Bandpass filtering (TR={tr}s, hp_sigma={hp_sigma}, lp_sigma={lp_sigma})...\")\n",
    "    !fslmaths {work_dir}/func_smooth -bptf {hp_sigma} {lp_sigma} {work_dir}/func_filtered\n",
    "    \n",
    "    print(f\"  ✓ Done: {sub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract functional connectivity features\n",
    "\n",
    "Parcellate with Schaefer 100 atlas and compute FC matrix, exactly as in Lab 4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract FC features using nilearn (same approach as Lab 4)\n",
    "from nilearn import datasets, maskers, connectome\n",
    "\n",
    "# Load Schaefer atlas (same as Lab 4)\n",
    "atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, yeo_networks=7)\n",
    "atlas_filename = atlas.maps\n",
    "labels = [l.decode() if isinstance(l, bytes) else l for l in atlas.labels[1:]]\n",
    "\n",
    "# Define the 7 Yeo networks\n",
    "network_names = ['Vis', 'SomMot', 'DorsAttn', 'SalVentAttn', 'Limbic', 'Cont', 'Default']\n",
    "network_ids = []\n",
    "for label in labels:\n",
    "    for i, net in enumerate(network_names):\n",
    "        if net in label:\n",
    "            network_ids.append(i)\n",
    "            break\n",
    "\n",
    "network_ids = np.array(network_ids)\n",
    "print(f\"Network assignment: {len(network_ids)} regions across {len(network_names)} networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FC matrix for each subject and extract network-level features\n",
    "fmri_features = {}\n",
    "\n",
    "for sub in all_subjects:\n",
    "    work_dir = f'/content/fmri_work/{sub}'\n",
    "    func_file = f'{work_dir}/func_filtered.nii.gz'\n",
    "    \n",
    "    if not os.path.exists(func_file):\n",
    "        print(f\"⚠️ Filtered fMRI missing for {sub}\")\n",
    "        fmri_features[sub] = [np.nan] * 28\n",
    "        continue\n",
    "    \n",
    "    # Parcellate with Schaefer 100 (same as Lab 4)\n",
    "    masker = maskers.NiftiLabelsMasker(\n",
    "        labels_img=atlas_filename,\n",
    "        standardize='zscore_sample',\n",
    "        memory='nilearn_cache',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        time_series = masker.fit_transform(func_file)\n",
    "        \n",
    "        # Compute FC matrix (Pearson correlation, same as Lab 4)\n",
    "        correlation_measure = connectome.ConnectivityMeasure(kind='correlation')\n",
    "        fc_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "        \n",
    "        # Extract network-level features: mean intra- and inter-network FC\n",
    "        n_networks = len(network_names)\n",
    "        features = []\n",
    "        feature_names_fc = []\n",
    "        \n",
    "        # Intra-network connectivity (7 values)\n",
    "        for i in range(n_networks):\n",
    "            mask = network_ids == i\n",
    "            intra_vals = fc_matrix[np.ix_(mask, mask)]\n",
    "            # Upper triangle only (exclude diagonal)\n",
    "            triu = intra_vals[np.triu_indices(intra_vals.shape[0], k=1)]\n",
    "            features.append(np.mean(triu) if len(triu) > 0 else np.nan)\n",
    "            feature_names_fc.append(f'Intra_{network_names[i]}')\n",
    "        \n",
    "        # Inter-network connectivity (21 values)\n",
    "        for i in range(n_networks):\n",
    "            for j in range(i+1, n_networks):\n",
    "                mask_i = network_ids == i\n",
    "                mask_j = network_ids == j\n",
    "                inter_vals = fc_matrix[np.ix_(mask_i, mask_j)]\n",
    "                features.append(np.mean(inter_vals))\n",
    "                feature_names_fc.append(f'Inter_{network_names[i]}_{network_names[j]}')\n",
    "        \n",
    "        fmri_features[sub] = features\n",
    "        print(f\"{sub}: {len(features)} FC features extracted (FC matrix shape: {fc_matrix.shape})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {sub}: {e}\")\n",
    "        fmri_features[sub] = [np.nan] * 28\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names_fc = ([f'Intra_{n}' for n in network_names] + \n",
    "                    [f'Inter_{network_names[i]}_{network_names[j]}' \n",
    "                     for i in range(len(network_names)) for j in range(i+1, len(network_names))])\n",
    "df_fmri = pd.DataFrame(fmri_features, index=feature_names_fc).T\n",
    "df_fmri.index.name = 'Subject'\n",
    "print(f\"\\nfMRI Feature Matrix: {df_fmri.shape}\")\n",
    "df_fmri.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. QC: Visualize FC matrix for one subject\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot FC matrix for one subject (same visualization as Lab 4)\n",
    "example_sub = all_subjects[0]\n",
    "work_dir = f'/content/fmri_work/{example_sub}'\n",
    "\n",
    "masker = maskers.NiftiLabelsMasker(labels_img=atlas_filename, standardize='zscore_sample', verbose=0)\n",
    "ts = masker.fit_transform(f'{work_dir}/func_filtered.nii.gz')\n",
    "fc = connectome.ConnectivityMeasure(kind='correlation').fit_transform([ts])[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(fc, interpolation='nearest', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "plt.title(f'Functional Connectivity Matrix - {example_sub} (Schaefer 100)')\n",
    "plt.colorbar(label='Pearson Correlation')\n",
    "plt.xlabel('Brain Regions')\n",
    "plt.ylabel('Brain Regions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VI. EEG Features (Lab 5)**\n",
    "\n",
    "We extract **spectral power** features from resting-state EEG using MNE-Python.\n",
    "\n",
    "**Pipeline:** Load data → Preprocessing (filter + ICA, as in Lab 5) → PSD computation → Band power extraction\n",
    "\n",
    "**Features extracted per subject:** Mean power in delta (1-4Hz), theta (4-8Hz), alpha (8-13Hz), beta (13-30Hz), and alpha peak frequency (APF) = 5 values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EEG preprocessing\n",
    "\n",
    "Apply the preprocessing pipeline from Lab 5: bandpass filter, ICA denoising with mne_icalabel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne_icalabel import label_components\n",
    "\n",
    "eeg_features = {}\n",
    "\n",
    "for sub in all_subjects:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {sub} - EEG\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Adapt path to your actual EEG file location and format\n",
    "    # LEMON EEG is typically in BrainVision (.vhdr) or EEGLAB (.set) format\n",
    "    eeg_file = f'/content/eeg_data/{sub}/eeg/{sub}_task-rest_eeg.vhdr'\n",
    "    \n",
    "    if not os.path.exists(eeg_file):\n",
    "        print(f\"  ⚠️ EEG file not found for {sub}\")\n",
    "        eeg_features[sub] = [np.nan] * 5\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load raw EEG\n",
    "        raw = mne.io.read_raw_brainvision(eeg_file, preload=True)\n",
    "        print(f\"  Channels: {len(raw.ch_names)}, Sfreq: {raw.info['sfreq']} Hz\")\n",
    "        \n",
    "        # Step 1: Bandpass filter (Lab 5)\n",
    "        print(\"  Step 1: Bandpass filter (1-40 Hz)...\")\n",
    "        raw.filter(l_freq=1.0, h_freq=40.0)\n",
    "        \n",
    "        # Step 2: Set average reference\n",
    "        raw.set_eeg_reference('average')\n",
    "        \n",
    "        # Step 3: ICA denoising (Lab 5)\n",
    "        print(\"  Step 2: ICA denoising...\")\n",
    "        ica = mne.preprocessing.ICA(n_components=20, random_state=42)\n",
    "        ica.fit(raw)\n",
    "        \n",
    "        # Classify components with ICLabel (Lab 5)\n",
    "        ic_labels = label_components(raw, ica, method='iclabel')\n",
    "        \n",
    "        # Remove non-brain components\n",
    "        exclude_idx = [i for i, label in enumerate(ic_labels['labels']) \n",
    "                       if label not in ('brain', 'other')]\n",
    "        print(f\"  Excluding {len(exclude_idx)} non-brain ICs: {exclude_idx}\")\n",
    "        ica.exclude = exclude_idx\n",
    "        raw_clean = ica.apply(raw.copy())\n",
    "        \n",
    "        print(f\"  ✓ Preprocessing done for {sub}\")\n",
    "        \n",
    "        # Step 4: Compute PSD (Lab 5)\n",
    "        print(\"  Step 3: Computing PSD...\")\n",
    "        psd = raw_clean.compute_psd(fmin=1.0, fmax=40.0, method='welch')\n",
    "        freqs = psd.freqs\n",
    "        psds = psd.get_data()  # (channels, freqs)\n",
    "        \n",
    "        # Average across channels\n",
    "        mean_psd = np.mean(psds, axis=0)\n",
    "        \n",
    "        # Step 5: Extract band power\n",
    "        bands = {\n",
    "            'delta': (1, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30)\n",
    "        }\n",
    "        \n",
    "        band_powers = []\n",
    "        for band_name, (fmin, fmax) in bands.items():\n",
    "            band_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "            band_power = np.mean(mean_psd[band_mask])\n",
    "            band_powers.append(band_power)\n",
    "        \n",
    "        # Alpha Peak Frequency (APF)\n",
    "        alpha_mask = (freqs >= 8) & (freqs <= 13)\n",
    "        alpha_psd = mean_psd[alpha_mask]\n",
    "        alpha_freqs = freqs[alpha_mask]\n",
    "        apf = alpha_freqs[np.argmax(alpha_psd)]\n",
    "        band_powers.append(apf)\n",
    "        \n",
    "        eeg_features[sub] = band_powers\n",
    "        print(f\"  ✓ Features: delta={band_powers[0]:.2e}, theta={band_powers[1]:.2e}, \"\n",
    "              f\"alpha={band_powers[2]:.2e}, beta={band_powers[3]:.2e}, APF={apf:.1f} Hz\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Error: {e}\")\n",
    "        eeg_features[sub] = [np.nan] * 5\n",
    "\n",
    "# Create DataFrame\n",
    "eeg_feature_names = ['Delta_Power', 'Theta_Power', 'Alpha_Power', 'Beta_Power', 'APF']\n",
    "df_eeg = pd.DataFrame(eeg_features, index=eeg_feature_names).T\n",
    "df_eeg.index.name = 'Subject'\n",
    "print(\"\\nEEG Feature Matrix:\")\n",
    "df_eeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. QC: Visualize PSD for one subject\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PSD for one subject (same style as Lab 5)\n",
    "# This cell assumes raw_clean from the last processed subject is still in memory\n",
    "# In practice, recompute for the subject you want to visualize\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "psd.plot(axes=ax, show=False)\n",
    "ax.set_title(f'Power Spectral Density - {all_subjects[-1]}')\n",
    "ax.axvspan(8, 13, alpha=0.2, color='orange', label='Alpha band (8-13 Hz)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VII. Feature Integration**\n",
    "\n",
    "Combine all features from the 4 modalities into a single feature matrix per subject.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all feature DataFrames\n",
    "df_all = pd.concat([df_mri, df_dwi, df_fmri, df_eeg], axis=1)\n",
    "\n",
    "# Add labels\n",
    "df_all['Group'] = label_names\n",
    "df_all['Label'] = labels\n",
    "\n",
    "print(f\"Combined feature matrix: {df_all.shape}\")\n",
    "print(f\"  MRI features:  {df_mri.shape[1]}\")\n",
    "print(f\"  DWI features:  {df_dwi.shape[1]}\")\n",
    "print(f\"  fMRI features: {df_fmri.shape[1]}\")\n",
    "print(f\"  EEG features:  {df_eeg.shape[1]}\")\n",
    "print(f\"  Total features: {df_mri.shape[1] + df_dwi.shape[1] + df_fmri.shape[1] + df_eeg.shape[1]}\")\n",
    "print(f\"  Subjects: {df_all.shape[0]}\")\n",
    "\n",
    "# Check for missing data\n",
    "n_missing = df_all.drop(columns=['Group', 'Label']).isna().sum().sum()\n",
    "print(f\"\\n  Missing values: {n_missing}\")\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions by group\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MRI: Example - L hippocampus volume\n",
    "if 'L_Hipp' in df_all.columns:\n",
    "    sns.boxplot(data=df_all, x='Group', y='L_Hipp', ax=axes[0, 0], palette='Set2')\n",
    "    axes[0, 0].set_title('MRI: Left Hippocampus Volume')\n",
    "    axes[0, 0].set_ylabel('Volume (mm³)')\n",
    "\n",
    "# DWI: Example - Genu CC FA\n",
    "if 'Genu_CC' in df_all.columns:\n",
    "    sns.boxplot(data=df_all, x='Group', y='Genu_CC', ax=axes[0, 1], palette='Set2')\n",
    "    axes[0, 1].set_title('DWI: Genu CC - FA')\n",
    "    axes[0, 1].set_ylabel('FA')\n",
    "\n",
    "# fMRI: Example - Intra-DMN connectivity\n",
    "if 'Intra_Default' in df_all.columns:\n",
    "    sns.boxplot(data=df_all, x='Group', y='Intra_Default', ax=axes[1, 0], palette='Set2')\n",
    "    axes[1, 0].set_title('fMRI: Intra-DMN Connectivity')\n",
    "    axes[1, 0].set_ylabel('Mean Correlation')\n",
    "\n",
    "# EEG: Alpha Peak Frequency\n",
    "if 'APF' in df_all.columns:\n",
    "    sns.boxplot(data=df_all, x='Group', y='APF', ax=axes[1, 1], palette='Set2')\n",
    "    axes[1, 1].set_title('EEG: Alpha Peak Frequency')\n",
    "    axes[1, 1].set_ylabel('Frequency (Hz)')\n",
    "\n",
    "plt.suptitle('Feature Distributions by Age Group (Example Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VIII. Classification: Young vs Old**\n",
    "\n",
    "We train simple classifiers using Leave-One-Out Cross-Validation (LOOCV) and compare unimodal vs multimodal performance.\n",
    "\n",
    "**Important:** With N=10, we expect noisy results. The goal is to demonstrate the process and discuss limitations, NOT to achieve high accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare data\n",
    "\n",
    "Handle missing values and normalize features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate features and labels\n",
    "y = np.array(labels)\n",
    "\n",
    "# Define feature sets per modality\n",
    "feature_sets = {\n",
    "    'MRI':  df_mri.values,\n",
    "    'DWI':  df_dwi.values,\n",
    "    'fMRI': df_fmri.values,\n",
    "    'EEG':  df_eeg.values,\n",
    "    'All':  df_all.drop(columns=['Group', 'Label']).values\n",
    "}\n",
    "\n",
    "# Impute missing values (mean) and standardize\n",
    "processed_features = {}\n",
    "for name, X in feature_sets.items():\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    processed_features[name] = X_scaled\n",
    "    print(f\"{name:>5}: {X_scaled.shape[1]} features, {np.sum(np.isnan(X)):>3.0f} NaN imputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Leave-One-Out Classification\n",
    "\n",
    "We use LOOCV because with N=10, k-fold would give very small test sets. Each iteration trains on 9 subjects and tests on 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Classifiers to try\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM (linear)': SVC(kernel='linear', random_state=42),\n",
    "}\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for feat_name, X in processed_features.items():\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        # LOOCV\n",
    "        y_pred = cross_val_predict(clf, X, y, cv=loo)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Modality': feat_name,\n",
    "            'Classifier': clf_name,\n",
    "            'Accuracy': acc,\n",
    "            'Correct': int(acc * len(y)),\n",
    "            'Total': len(y)\n",
    "        })\n",
    "        \n",
    "        print(f\"{feat_name:>5} + {clf_name:<25}: {acc:.1%} ({int(acc*len(y))}/{len(y)} correct)\")\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot classification accuracy by modality and classifier\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "pivot = df_results.pivot_table(index='Modality', columns='Classifier', values='Accuracy')\n",
    "# Reorder modalities\n",
    "order = ['MRI', 'DWI', 'fMRI', 'EEG', 'All']\n",
    "pivot = pivot.reindex([m for m in order if m in pivot.index])\n",
    "\n",
    "pivot.plot(kind='bar', ax=ax, colormap='Set2', edgecolor='black', width=0.7)\n",
    "\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='Chance level (50%)')\n",
    "ax.set_ylabel('Accuracy (LOOCV)', fontsize=12)\n",
    "ax.set_xlabel('Feature Set', fontsize=12)\n",
    "ax.set_title('Classification Accuracy: Young vs Old\\n(Leave-One-Out, N=10)', fontsize=14)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.0f%%', label_type='edge', fontsize=9,\n",
    "                 padding=2, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the best multimodal model\n",
    "best_row = df_results[df_results['Modality'] == 'All'].sort_values('Accuracy', ascending=False).iloc[0]\n",
    "best_clf_name = best_row['Classifier']\n",
    "best_clf = classifiers[best_clf_name]\n",
    "\n",
    "y_pred_best = cross_val_predict(best_clf, processed_features['All'], y, cv=loo)\n",
    "cm = confusion_matrix(y, y_pred_best)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Young', 'Old'],\n",
    "            yticklabels=['Young', 'Old'], ax=ax, cbar=False, annot_kws={'size': 16})\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('True', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix - Multimodal ({best_clf_name})\\nAccuracy: {best_row[\"Accuracy\"]:.0%}', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature importance analysis\n",
    "\n",
    "Which features (and modalities) contribute most to classification?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression on ALL features to get coefficients\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_final = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_final.fit(processed_features['All'], y)\n",
    "\n",
    "# Get feature names\n",
    "all_feature_names = (list(structures) + list(jhu_tracts.keys()) + \n",
    "                     feature_names_fc + eeg_feature_names)\n",
    "\n",
    "# Get absolute coefficient values\n",
    "coefs = np.abs(clf_final.coef_[0])\n",
    "\n",
    "# Assign modality to each feature\n",
    "modality_labels = (['MRI'] * len(structures) + ['DWI'] * len(jhu_tracts) + \n",
    "                   ['fMRI'] * len(feature_names_fc) + ['EEG'] * len(eeg_feature_names))\n",
    "\n",
    "# Create importance DataFrame\n",
    "df_importance = pd.DataFrame({\n",
    "    'Feature': all_feature_names[:len(coefs)],\n",
    "    'Importance': coefs,\n",
    "    'Modality': modality_labels[:len(coefs)]\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top15 = df_importance.head(15)\n",
    "colors = {'MRI': '#3498DB', 'DWI': '#E74C3C', 'fMRI': '#2ECC71', 'EEG': '#F1C40F'}\n",
    "bar_colors = [colors[m] for m in top15['Modality']]\n",
    "\n",
    "bars = ax.barh(range(len(top15)), top15['Importance'].values, color=bar_colors, edgecolor='black')\n",
    "ax.set_yticks(range(len(top15)))\n",
    "ax.set_yticklabels(top15['Feature'].values, fontsize=10)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Absolute Coefficient', fontsize=12)\n",
    "ax.set_title('Top 15 Most Important Features for Classification', fontsize=14)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, edgecolor='black', label=m) for m, c in colors.items()]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: average importance by modality\n",
    "modality_importance = df_importance.groupby('Modality')['Importance'].mean().sort_values(ascending=False)\n",
    "print(\"Average feature importance by modality:\")\n",
    "print(\"=\" * 40)\n",
    "for mod, imp in modality_importance.items():\n",
    "    bar = '█' * int(imp / modality_importance.max() * 20)\n",
    "    print(f\"  {mod:>5}: {imp:.4f}  {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IX. Discussion and Limitations**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to address in your discussion:\n",
    "\n",
    "1. **Results interpretation:**\n",
    "   - Which modality performed best individually? Does this match your expectations based on the neuroscience literature?\n",
    "   - Did the multimodal approach improve over the best unimodal approach?\n",
    "   - Which specific features were most discriminative?\n",
    "\n",
    "2. **Limitations (be honest!):**\n",
    "   - With N=10, how reliable are these accuracy estimates? What is the expected variance?\n",
    "   - What is the risk of overfitting, especially with the multimodal feature set?\n",
    "   - How does the LOOCV strategy help or hurt in this context?\n",
    "   - What preprocessing choices could have affected the results?\n",
    "\n",
    "3. **What would you do differently with more data?**\n",
    "   - How many subjects would be needed for reliable classification?\n",
    "   - What additional preprocessing or feature extraction methods would you apply?\n",
    "   - Would you use different classifiers or feature selection methods?\n",
    "\n",
    "4. **Process reflection:**\n",
    "   - Which step in the pipeline was most challenging? Why?\n",
    "   - Where did you have to make methodological decisions? How did you decide?\n",
    "   - What quality control checks were most informative?\n",
    "\n",
    "---\n",
    "\n",
    "**Write your discussion below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your discussion here (use markdown cells above or print statements)\n",
    "print(\"TODO: Write your discussion addressing the questions above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **X. Summary**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"=\" * 70)\n",
    "print(\"PROJECT 10 - MULTIMODAL AGE CLASSIFICATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset: LEMON (MPI-Leipzig Mind-Brain-Body)\")\n",
    "print(f\"Subjects: {len(all_subjects)} ({len(young_subjects)} young, {len(old_subjects)} old)\")\n",
    "print(f\"\\nFeatures extracted:\")\n",
    "print(f\"  MRI  (Lab 1): {df_mri.shape[1]:>3} features (subcortical volumes from FIRST)\")\n",
    "print(f\"  DWI  (Lab 2): {df_dwi.shape[1]:>3} features (FA from JHU tracts)\")\n",
    "print(f\"  fMRI (Lab 4): {df_fmri.shape[1]:>3} features (network-level FC from Schaefer)\")\n",
    "print(f\"  EEG  (Lab 5): {df_eeg.shape[1]:>3} features (band power + APF)\")\n",
    "print(f\"  Total:        {df_mri.shape[1] + df_dwi.shape[1] + df_fmri.shape[1] + df_eeg.shape[1]:>3} features\")\n",
    "print(f\"\\nBest results (LOOCV):\")\n",
    "for _, row in df_results.sort_values('Accuracy', ascending=False).head(3).iterrows():\n",
    "    print(f\"  {row['Modality']:>5} + {row['Classifier']:<25}: {row['Accuracy']:.0%}\")"
   ]
  }
 ]
}